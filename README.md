## 1. Introduction

Interactive-CV is a **Web Application**, showing a virtual pseudo-terminal emulator with a command line interpreter, sitting on top of a virtual file system, allowing for any user to interact and read data off from an *"old-fashioned"-themed* console. The content provided by the App is specifically for CV-oriented content. That is, the deployed App holds data corresponding to anything that the user might find useful in his/her Curriculum Vitae.

In order to understand the application in more depth, we have to dive a bit into the world of computer consoles and terminals. Nowadays, any ``UNIX``-like operating system (and even Windows), e.g. GNU/Linux, FreeBSD, SunOS, Solaris, etc. have some sort of a command line interpreter. A *CLI* is just a running program that takes in any user input, talks to the kernel (that would be the heart of the OS) with that input, and gives back some response, generated by the **Operating System**. These systems have several programs that seem to be doing the same thing, but in reality, are different. We have to differentiate them, in order to get the idea of this project.

First of let's talk about a terminal emulator. That is simply a program that communicates with its host computer system through serial lines using some character set (usually the full ASCII char set) and control sequences. For instance, the famous [VT100](https://en.wikipedia.org/wiki/VT100) is one such terminal emulator. But as technology progressed, **pseudo-terminal emulators** came in. They simulate the work of a terminal-emulator. Real world examples would be `cmd`, `gnome-shell`, `xterm`, `konsole`, etc. Again, these two things are separate programs. On a host machine, we can even access both of them! Throughout the paper, I will be referring to a *pseudo-terminal emulator* simply as a **terminal**, since it's quite a verbose term. Interactive-CV in reality is showing a **pseudo-terminal emulator**. But what about the *shell*? Well, a shell is a Command Line Interpreter, it is also another program that talks to the kernel. Examples of shells are `zsh`, `bash`, `fish`, `powershell`, etc. So when we combine a shell (i.e. some form a CLI) and a pseudo-terminal emulator, we get a fully working station that can do real work! Interactive-CV, apart from having a pseudo-terminal emulator, has a very basic shell. Actually the shell can be seen as the Controller between the terminal and the underlying system. Underneath the CLI, there sits a Virtual File System. It interacts with the files that the user has provided as *"workable data"*. The **Virtual File System** emulates an Abstract Data Structure (**ADT**), more specifically a `B+ Tree`, that holds all of the contents saved on the physical disk of the host, and sends it to the shell, which in term, shows it off to the user. I will be using the abbreviations **`VFS`** for Virtual File System, **FS** for File System, **CLI** for Command Line Interpreter, and **JS** for JavaScript, throughout the documentation, since they are sometimes quite verbose to spell out in full.

The technologies that are used in the project are as follows - `vanilla JavaScript`, `Node JS` and `Express JS`. The pure JavaScript is used to create the *Virtual File System*, the *CLI*, the *User Interface*, it's used to connect the input of the user to the server, and to represent any received answers to the user back. In addition to this, all of the Algorithms and Data-structures, business logic, and formatting have been written in pure JavaScript. The Node JS framework is used to build a server that can support the Web App, render its contents, talk to the actual File System, and manage the state between the *"front-end"* and *"back-end"*. In addition to Node, the Express JS library is used to make the support of the running server simpler. Express JS allows for a lighter and easier setup of the Web App. Since the whole application is a single page that does it all, no complex server manipulations are needed, hence the use of a simple case for Express JS. Although the application, as already mentioned, is in a single web page, it still uses standard `HTTP` `GET/POST` methods and is technically utilizing the *RESTful* way of things.

There are two main reasons I have chosen to create such a project.

First one being that I find standard CVs boring and cliche. Putting all your work into a single sheet of paper, where you briefly list your qualities is not enough, especially for any technical evaluations. It's more important to show-off projects and written code and how it works, more than simply stating your qualifications. Tech companies get a better grip of the candidates capabilities when they have a direct contact with the applicants projects. They build a better representation of how they work and show more interested in what you have done. So with this project, I want to stand out a bit more from the crowd and be able to represent myself in an original way. I want to provide a fun area that will bring a lighter look on candidate evaluation and even help out both the candidate and the recruiter.

The second reason for my choice is because I find both concepts fascinating. In the past I have developed large and small scale `bash` scripts and wrapper programs that directly talk to the host OS. Handling user input and command line arguments is a challenge. Taking that to the browser shifts that challenge to new grounds, since the user input might have catastrophic impact on the host machine, if that input isn't handled correctly. But making only a shell and a terminal is like having a painted egg, but hollow on the inside. That is why I've turned to File Systems. In modern day OS's, the File System takes a third of the whole inner working. The input is parsed, passed down to the Operating System, and then an appropriate response is generated. A Virtual File System is an abstract layer, sitting on top of a concrete File System, that manages the state of the physical representation of data onto something that a user can manage and work with. Emulating these two things brings great insight into the inner workings of an Operating System. Both rely on complex Data Structures that can provide speed and readability to the user. Diving into this world of emulation allows for a fun trip (partially) into the world of low-level programming. One of my goals as a software developer is to one day write kernel code. I find that this project is a great way for me to practice basic concepts that would have to become second nature to me in due time.

![vt100](./assets/vt100.jpg "vt100"){ height=42% }
Figure 1: VT100

This is an image from an old VT100 machine. We can see a *classy* command line prompt on the screen.

## 2. Specification and Analysis of the Software Requirements

The specification of the Interactive-CV is separated into *functional* and *non-function* requirements. The aim of the functional requirements is to describe what the software **should do**. Most of them aim at simplicity and ease of use of the application. The non-functional, on the other hand, try to provide a fast, extensible and stable experience to the user.

First, let's start with the analysis of the functional requirements. The software *should*:

- *Have a simple and readable command line interfaced*. This means that the command line interpreter must be easy to use and read. Things like helper functions, explanations of inner workings, documentation, easy setup, and easy readability, **must** be present in order for a person who has never touched such an interface, to feel comfortable when interacting with the application for the first time. Helper functions are those that try to guide the user through what the deeper functionalities of the app. They don't contribute to the actual workings of the CV, but rather try to hold the hands of first timers with playing with this interface. In addition to this, documentation and inner workings should be provided, if someone want's to try to manipulate the structure of the app in a more create way. This documentation should be widely available and easy to read. Of course, part of the simplicity is also to be able to setup this whole project from scratch. Having a starting guide would prove to be helpful to anyone who want's to benefit from such a project.

- *Respond to all types of inputs*. Part of the fun is to explore the unexplored territory. Taking the fun out of tinkering with a new project and discouraging such behavior is not part of this CV. The software has to have a responsible way of handling all types of user input, allowing for *"play in the terminal"*. This doesn't mean that all types of data should be validated, but that for every input, there is a logical response that will keep the user searching for more. This would incentivize the user to dig deeper and look carefully for what the candidate has written!

- *Be able to add text data to files*. The structure of the application must hold data, that would be the contents of a CV, and be able to manipulate that data in a meaningful manner. Any user should be able to view the text that is in each file, and also be able to alter it, append to it, remove it, or even overwrite it. File management is expected to be an easy task for anyone who is using this application. This includes in itself the following sub-functionalities. When a file is *deleted* through the user interface, then that file must no longer be present in the current running instance of the application, but that file on the physical disk **must not be gone**. When a file is created during the user interface session, that file *must* continue being present until the next restart of the application. The newly created file **must not** be also created on the physical medium of the host. If a file is to be altered in any way by the user, then that file must hold the new changes until the next restart of the application and **must not override** the original contents of the file. If we extend this functionality to also include working with directories, the needs would be identical. No matter if we create, delete, or alter any directories, they must be consistent with the currently working instance of the application, but once the Interactive-CV is restarted, then all data must return to it's original state.

- *Be unbreakable*. This means that any input that the user sends, must not break the system. This requirement holds its place, because the underlying implementation of the application is emulating a file system. If the file system were to break, that means that the whole application would fall to the ground. That is why any input must be handled with care. Such an implementation must be done on several layers of the system, each with it's own meaningful resources and error-handling.

And here are some of the non-functional requirements. They focus on usability, efficiency, quality, speed, etc. These things focus on **how** the software *should*:

- *Be able to make a response to a command in less than 2 seconds*. Since the whole structure of the application is meant to be in `RAM`, then there is nothing stopping it from being fast with the responses from the server. Any slower operations would cause the user to flee from a slow system, that can't render simple text based content. In order to meet this need, optimal structures of data and algorithms should be used, both on the "front-end" and "back-end" of the system. Logarithmic access and retrieval times are to be expected. In addition to this, the application should not take more than a couple of seconds to start or be restarted.

- *Have high code quality*. Software craftsmanship is highly valued. It's expected that after the end of the project, maintenance and further improvements will be continually made. If the code is not structured and prepared in such a way, that allows for further expansions and easy bug-fixing, then that would provide for a poor project life. This means that the code base should have correct comment sections, code documentation above each method, perfect separation of concerns, a valid architectural model that is followed to the end, diagrams that help understand the project and a list of future works to be done. If this is not met, then neither the original creator, nor any following contributor would want to maintain this software and any piece of code that is not regularly maintained, falls of the market and becomes useless.

- *Non-intrusive help from the User Interface*. It is very important that any form of help that comes from the system to be as inconspicuous as possible. User friendliness is achieved when the user thinks he/she have discovered something on their own. This should be done through layered error messages, that are generated on each step of processing, allowing for a decoupled, yet understandable explanation of the situation. In addition to this, all of the helper functions should have hints as to how to make the functionality of the project more approachable.

\ \

### Use cases
The use cases of this application are for the people who are interested and looking at the CV of a person through a new an interesting interface, and for those who want to see how a Virtual File System works/is implemented. Uses cases include what might happen at any point during program operation and how that would resolve in the overall experience. This includes different types of input statements, access data, etc.

We can look at things in a diagrammatic form:

![use-case](./assets/usercase.jpg "use-case"){ height=50% }
Figure 2: Use-Case

From the diagram we can see how the parties interact with each other. We have a user, that makes requests to the server of the application (`POST` to be exact). Now, the user doesn't know that he/she are making such requests, since the *UI* hides part of that magic. First we see that a command, any text that is, is entered. Following that is an interpretation of the input, which is received by the server. At that point the server makes a decision as to what to do exactly. It can either decide to manipulate the `VFS`, if it's required by the passed command, or it can continue executing some other command or functionality. Picking the command and executing it are two different things, since some commands do additional computation and manipulation of data, while others just generate text that is `echo`-ed back at the user or show the current time. Since manipulation of the `VFS` is an autonomous operation, we wait for it to finish. Then we can proceed with the generation of some form of a response which goes all the way back to the user. We can also see that the server does some manipulations with GitHub since the start of the application. This takes place as soon as the server loads the user profile, reads the data from the configuration, and then access the GitHub open API to read data from the users Account. This a simple `GET` command that is then re-interpreted as a response from GitHub. At this point, the received data, in `JSON` format, is evaluated and added to the user profile and generated from the start of the application. Thus we have a complete cycle of operation and expected execution for the uses cases that are expected to take place.

It would also be a benefit to look at things in a pipe lined way. This would allow for the information to be shown in a streamlined way.

![pipeline](./assets/pipeline.jpg "pipeline")
Figure 3: Pipeline

From this diagram, we get another sense of how the *"data flows"*. From the diagram we abstract the concept of the user, since we treat that entity as a final destination and resource generator. As we can see, each part of the application, or rather, each component has a more layered aspect to them. At each point of manipulation, we have a way of getting back at the user. This accounts for the usability and readability of the application. Form this part we can see that we are providing the user with a clear response that is both informative and gives incentive. Why is this so? Because the app is allowing for the user to take any action, and the consequences of that would be clear. What this means is that, when we have an invalid command, that would be rendered immediately after it doesn't pass the parse check, the user would be aware of that, he/she will learn, and continue experimenting until they get it right. This is meeting some of the requirement criteria that were set out to be fulfilled.

Let's go quickly through the pipeline and see how things are handled. First we take in any text and we parse it on the client level, which is closest to the user. If that check fails, we generate text that explains the situation and we allow for the client to try again. If this passes, then we try to execute the command. Each command varies in complexity and execution, that's why we have to verify it's correct. If the command is not in the set of executable operations, or if the command itself has failed, then we invalidate this whole step, and we generate a response. Finally, if all is gone and if the specific command needs to access this level, we go to the depths of the tree interface, the `VFS`, and manipulate our data here. We either add, delete, or modify our files and or directories, then we go back, step by step, generating a proper response all the way back up to the users interface. The interesting thing about this diagram, is that is hows the cyclic nature of the app. We don't have a blocking system here. Everything that comes in, must, in one way or another come out. Let's imagine that our own system just blocks for every small mistake we make, and we are forced to restart everything. This obstacle is overcome in order to allow for a smooth user experience. Not only that, but this is also a good software practice to make a system durable, even during erroneous executions.

Finally, let's look at some of the activities that are provided in this app.

![activity](./assets/activity.jpg "activity"){ height=70% }
Figure 4: Activity

This diagram give us yet another view of the processing that is done. Although we have an start and end state, this is only to show how a single input would be handled by the system. Of course the cyclic nature still persists. Now let's understand what this diagram is showing us. First we have some form of a web access, this is initiated by the user simply by visiting the web application. After that, a command is typed in and parsed. Here we have the fist major decision - what to do with this input. Well, if the input is *incorrect*, then we send an error message back, and we opt for another try to enter a command. If it's *correct* however, then we send that command to the server. On it's own, the server also does a parsing check, making sure it understand that received command. Again we have a generation of an error message that throws us back at the command line prompt for another try. If the server understands the command and executes it, then there are several things that can happen. We can simply execute the command and be done with this whole process. Or we can access the `VFS`, do some work there, finish the command execution and then be done with things. Or we can also read the data from the remote access and then return a response to the user. The end of this whole process finishes when we have a valid response sent back. We need an end state in order to see when we have finished our work, otherwise we would be trapped in a loop and won't understand when we have finished one command and started the other.

## 3. Design of the software solution
The application architecture is a highly modular system, where a central framework is working in the sever part, caters to the user input, and can be extended through many small modules that can be *"plugged-in"* to it. The framework is the virtual file system. To it, there are commands that interact with it and provide the content to the user. Without that system, the commands cannot work, but with it, anything can be plugged-in as a command and it can have some purpose to the overall application.

### Abstract Data Structures

The core of the whole application is part of the server side, namely - the **Virtual File System**. So the question is, how do we build a Virtual File System? How does a regular file system work? These two things are different programs that play a key role in the workings of any OS. A normal file system is just a piece of software that manages how bits are stored on a physical disk, be that a mechanical disk, or a solid state drive. If we manually try to install an operating system, for instance trying to install `GNU/Linux` from scratch, then we are given the opportunity to partition our physical disk manually, and then, through a command `mkfs.XXX` we apply a certain type of file system to our OS. This means that the `GNU/Linux` kernel implements several types of file systems that come with any distribution. For instance, we have things like `ext4`, `ntfs`, `btrfs`, `zfs`, all of them use some type of data structure to write data to a disk. `Btrfs` implements a [Merkel Tree](https://en.wikipedia.org/wiki/Merkle_tree), `zfs` does something similar with snapshots. `Ext4`, `Ext3`, and `Ext2` all implement a *B+ tree*. On the other side, a `VFS` is another piece of software that abstracts one layer above the actual FS, with the idea that regardless of what the actual FS lies underneath, the `VFS` can handle any data input and it will contact the real FS and save the data. So the reason to have a `VFS` is to have an abstraction that will give an easy interface to the user to play with. A mapping of all of the data will make it easier to interpret and will allow for a cross platform communication between incompatible FS types, such as the `NTFS` and `ExtX` ones. In the `GNU/Linux` distribution, there exists a `VFS` that is one layer above the real FS and allows for the user to write and read from the disk without dealing with partitioning and saving data to different sectors. The question now lies, what kind of structure does a `VFS` have? Of course there are many implementations of it, and I have chosen to create a custom *ADT*, which is on the basis between a `B+ Tree` and a `Doubly Linked List`. Since no such structure really exists, I've named it an `M-Tree`.

![m-tree](./assets/mtree.jpg "m-tree")
Figure 5: M-Tree

Here is the basic layout of the Tree. We have a root node, as with a normal B+ Tree, where below it, we find many nodes and leaf-nodes connected to the root. There are a couple of key differences we see right away. For one, there is only one root which has a lot of children. This leads us to believe that the M-Tree doesn't seem to have several constraints a normal tree would, but there is a reason for that!

First we have to understand what kind of a problem we want to solve with the `M-Tree`. Since it will be the core of the `VFS`, we want it to be able to emulate some sort of a real FS. Since I am highly influenced by the `UNIX`-type of file systems, I have implemented a tree structure that simulates the `UNIX` FS tree as well. What that means is that I will have a **root**, i.e. a base of the tree, which will be able to hold many files and sub-directories. Since the `M-Tree` is a variant combination between a `B+ Tree` and a `Doubly Linked List`, there are a few differences between the different two ADTs. They are as follows:

**1 -** A `B+ Tree` has constraints on it's child nodes. This is done so that it has a balanced data load. When a `B+ Tree` is implemented as a database or a FS structure, it's important for the data to be balanced across physical space, thus minimizing the access operations and maximizing space utility. Typically, a standard implementation of this type of tree would require to have a predefined height - `t`. In consequence, every node can contain no less than `t - 1` keys, except the root, which can contain as little as 1 key. And every node, including root, can have at most `2t - 1` keys. Also all of the keys are stored in an increasing order. But since the `M-Tree` would be doing a `VFS` mapping, and not a real one, it does not need have these constraints on data size and load balance. Since the application is completely loaded into RAM, there is not concern with any I/O operations to some physical space on the disk. Working with random memory also allows the structure to have virtually instant access to all of the data. So this data structure does not hinder from the fact that it won't have the constraints as the ones in the `B+ Tree`. Furthermore, the lack of such constrains allow for a simpler mapping of the actual files and directories, which will be represented in the UI.

**2 -** A `B+ Tree` is heavy and balanced. What that means is that the `B+ Tree` is made as a more massive structure, to handle large amounts of data, to minimize writing and reading operations from disk, and to be able to do such operations fast. The worst case scenario for a `B+ Tree` upon search/insert/delete actions is `O(log(n))`. The `M-Tree`, with it's lack of constraints, does not guarantee an any data balance and it's average case scenario for search/insert/delete would be `O(log(n))`. But there is a catch - this structure is not made to handle the amounts of data a real `B+ Tree` is meant to deal with. The reason for that is because the `M-Tree` is part of an online web app, and if we are to pass through this Web App gigabytes of data, the browser would have crashed by the time the `M-Tree` becomes unable to handle that data. Not to mention that such functionality is not implemented for safety reasons. In addition to this precaution, the `M-Tree` is also completely working in RAM. This guarantees almost instant access to any elements, without the fear of having to slow down I/O operations.

**3 -** A `B+ Tree` is traversed from root to the end nodes. One thing that a normal `B+ Tree` does for any operation, is to start from it's root node, and do either a Breath First or a Depth First search. This guarantees logarithmic time of access to any single node in the structure (couple this with it's balancing mechanism and we are guaranteed logarithmic times). But the `M-Tree`, since it's not balanced by nature, has another trick. It is also made to have both lateral and horizontal access to all child, parent, and sibling nodes. Now what that means is that, there is not only a side access to all nodes, such as a `B+ Tree` would have access to any sibling nodes, but there is also lateral movement up and down the tree, to all parent and child nodes. This sort of functionality mimics the way a doubly linked list would act, with the back and forth data access. The reason this was implemented was because virtual file systems have a concept of **relative paths**. Now there are several approaches as to how to tackle relative paths, but having a complete relative access to all nodes from any give point, server as a good solution. The reason it's a good solution is because it can cut down on search times. Since another approach would be to brute force every single possible path to the relative destination, this proves to be the more optimal and coherent way to do things.

**4 -** A `B+ Tree` has nodes as leaf nodes that hold simple valued data. This is done so that that the tree can be balanced on the basis of some numerical values, such as the size of bytes any single file holds (if we are to take a real `B+ Tree` implementation for a file system). The `M-Tree` also has nodes, serving as leaf nodes, which are aptly named - `inodes`. The reason they are called like that, is because in real life FS, the files are referred to as `inodes`. Keeping this similarity in the naming convention helps with the understanding of the implementation. I will use the terms `inodes` and files interchangeably. What is the difference between normal leaf nodes and `inodes`? On the first glance, nothing is stopping the `M-Tree` to have normal, numerically labeled leaf nodes, since that is just an object which can hold any arbitrary piece of data and just have one data filed to be used as a numerical representation, but since this `VFS` is doing a mapping of the files and directories, I chose to not have such numerical sorting on the files. The reason for this is simple. On one hand, this makes the implementation of the `VFS` simpler, and does not hinder it's working capabilities. Secondly, I think it's much more logical for a `VFS` to be concerned with what the contents of a file are, than how and where in reality these files are to be found on the actual medium of storage. Thus, the `inodes` that are implemented, hold the following data inside - the name of the actual file and its contents in plain text. The reason why there isn't any implementation on where the files is this - it's location in the `VFS` can be found out simply through the searching algorithm, that type of data does not need to be stored, since there's just easy access to it. The reason why the inode only stores plain text is because that's the whole meaning of the project. Storing any form of binary data, i.e. any form of executable content, will put the host machine in risk of containing malicious software. Of course that does not stop the user to fill in plain text code that can be read as malware, but another added security feature is that this piece of text is never really executed. It is read and retrieved through the browsers, but in reality, that text is never run through a real interpreter or code parser. This allows for a much more secure way of storing data and not having to worry about someone filling it with ill-intended code. Of course this does limit the owner of the CV to represent him or herself through plain text and links, but that is a small price to pay for security. Besides, a CV is mostly text, so no real loss of data here!

![mtree-close](./assets/mtree-close.jpg "mtree-close")
Figure 6: M-Tree Close

Looking at the tree from up close, we can spot the differences better. We can see that all of the tree objects, both the root and the children, can have an expanding number of files in each tree. Furthermore, we also conceptualize that one directory comprises of two things, a tree object, which would hold both subsequent directories, and one or more `inodes`. In this specific picture, it's demonstrated that there is only one **root**, holding a couple of files and one child directory. The child dir, named `expr`, also has a couple of files and **3** sub-directories. Each of which hold many files, are called differently, and are also *siblings* to one another. This would mean that `./skills`, `./langs` and `./proj` are siblings, sharing one parent - `./expr`, and one great parent - `root`. The cool thing about this, as we will see shortly, is that all of these places and files can be reached in a relative and optimal manner.

Having in mind all of these differences and similarities between the three data structures, we can further look at how the actual `VFS` is built on top the `M-Tree` and `inodes`. Since this is just an abstraction of a data structure, we need something that binds it all together to make it a coherent `VFS`. That's why there is another class, called `jayVFS`, that wraps around this ADT and implements an inner working API, that allows for the creation, deletion and searching and retrieving of files and directories. In addition, this class also maps the directories that the user has specified, and returns some other needed information, such as the *root node*, the contents of any specified directory, the currently visited directory, and many other functionalities. So we can see that this wrapper has generic and some specific capabilities. At this point we have to understand, where do the requirements for these functionalities come from. Well they come from the standard set of `UNIX` commands that are present in any distribution. When we open any terminal emulator, we have a wide range of commands are our disposal that we can play around with. Most of them either run another program, or they talk to the kernel directly, or they talk to the FS. In this case, since the data is stored on an emulated FS, it makes sense that the commands that are implemented, would also interact with the `VFS`. In term, here are the set of commands that have been developed to work with the whole structure:

`ls` — Stands for "list". lists directory contents. \
`rm` — Stand for "remove". Remove a target file. \
`cd` — Stands for "change directory". Changes the working directory. \
`pwd` — Stands for "print working directory". Prints name of current/working directory. \
`cat` — Stands for "concatenate". Concatenates files and print on the standard output. \
`date` — Stands for "date show". Prints or set the system date and time. \
`echo` — Stands for "echo back". Displays a line of text. Can also add or append data to a file. \
`help` — Stands for "print help". Prints help, or, if specified, prints a specific help listing of a command. \
`mkdir` — Stands for "make directory". Makes a specified directory. \
`rmdir` — Stands for "remove directory". Removes a specified directory. \
`clear` — Stands for "clear the screen". Clears the screen from all text. \
`touch` — Stands for "create or touch a file". Creates a file. \
`whoami` — Stands for "who is the creator of this". Prints out personal data for the owner of this CV.

All of these commands are present in any standard `UNIX` distribution. Most of the listed here commands have the same functionality as they would in a real OS, but some have a twist to them. First we will look at the commands that directly affect the `VFS` - these are ls/rm/cd/pwd/cat/echo/mkdir/rmdir/touch. In order to see how they work we have to get into the next topic, which would be the *algorithms* of the project.

### Algorithms

The application has several *algorithms* that are present. All of them server to make the whole experience as smooth as possible, where speed and simplicity of use is always pursued as goals. Some of them are small and simple, some quite big. Not all of the algorithms in this section are related to the `VFS`, some are used to clean data, some to parse user input, and some, in fact, do deal with the `VFS`. Here all of the commands that are supported will be explained. In addition to this, the remote access of other services will be examined, as well as path resolution and user input handling will be *debunked*.

Let's start with the ones that interact with the file system. These would be the tree traversal algorithms as we all as all of the commands that have been listed in the previous section.

- **Tree traversal**: Since the virtual file system is a tree, a traversing algorithm is used to find a specific `inode` or directory that will be reached. It works like this - a string, composed in a way to a destination, is passed to the algorithm. That string is the subsequently split into an array of literal strings, comprising the target directory or file. Then for each sequential part of the whole array, starting from its first element, is checked whether that element exists as a `sub-tree` in the currently visited directory, starting from the root node. If such a sub-directory exists, then the tree object moves to that node and repeats the same task with the remaining destination arguments of the array. If no such directory exists, then an error is throw, with the appropriate message. Since the tree is mimicking a `B+ Tree`, it's average case for traversal is `O(log(n))`. This is the generic way of traversing the `M-Tree` and as we can see, it has a recursive nature to it. In the following commands, more concrete examples will be given as to how exactly this tree traversal is done.

- **Relative path resolution**: Another key algorithm of the application is the ability to resolve any path as input, and manage to navigate to the desired location, *if* it exists. The relative path resolution is what motivates the `M-Tree` to have both *lateral* and *horizontal* navigation from any node point. The algorithm works as follows - it takes in a simple string argument. First it does an absolute path check. An absolute path is any path that starts from the root node and has some end destination, which is logically connected to the root in a valid manner. Basically, if the passed argument does indicated such a path, then it just skips the whole resolution and passes it on to the appropriate command. We can check if a path is absolute or not, by checking if the passed argument starts with a forwards slash - `/`. The logic behind this is that, even if we are at root level, that doesn't mean we are not going to use a relative path. So the only way to be sure that the string indicates something absolute is, if it starts with that `/` symbol. If this test fails, then we have to navigate appropriately to the needed destination. Relative path navigation uses two specific symbols - a single or a double dot, e.g. '.' would represent the current directory and '..' would represent the parent directory. Any child directory representation is made via the child directory name. There are **eight** cases in total to be handled here:

1. The path starts with a single dot (`.`). This would force us to start working from our currently visited directory.
2. The path starts with a double dot (`..`). This would force is to start working from our parent directory, prior to our current one.
3. The path starts with a forward slash and a dot (`/.`). This indicates that we want to start from the absolute path of the tree and then access it's current contents. This would be logically equivalent to `/`.
4. The path starts with a forward slash and a double dot (`/..`). This indicates that we want to start from the root path of the tree and then go out step above it. Although this makes no practical sense, it is still a valid way to represent a path in a `UNIX` system.
5. The path is blank. If we get a blank input we have to assume that this means that the path starts from the currently visited directory.
6. The path starts with a name of a file or directory. If this is the case, then as the above situation, we make the assumption that we are working from the current directory and then we continue on to resolve the rest of the path. There is a caveat here tho - if we are in the **root** level, then we must explicitly tell the algorithm to start traversing from it, otherwise it would get confused as to which is the current working dir.
7. The path starts with a forward slash and a word (`/XXX`). This indicates that we want to start out traversal from the *root* node and then continue on from it.
8. The path starts only with a forward slash (`/`). This is simply indicating the *root* node as a starting point.

All of these things would make up for very strange use cases, where simultaneous forward and backwards navigation would be required. Depending on the position of the relative dots notation, we simply move for each part of the relative path array, to the desired directory, and then we repeat this step for every element till the end of the array. In the end we get a *full path*, that navigates to the desire directory, *if* it exists. Here we are utilizing the ability to see what the parent, or the child of the parent node is. This backwards and side peek of a sub-tree, allows us to quickly navigate to a nearby destination, without having to traverse the whole tree several times from the **root**, in order to find the needed directory. A good question would be - how do we know the concept of a parent and sibling node? We know this information in the tree, because, every time we *create* a directory, that `sub-tree`, the object that holds more directories and files, has both a parent node and it's full path as data members. This allows us to access that information on the go, without having to compute it each time. Only once is enough, when we create the needed dir (Although it's not even computed then, since we just retrieve it with each recursive iteration).

Let's take an example. If the user would pass this argument `/experience/projects/` and if we taken it as a given that all of the directories exist in that argument (namely `experience` and `projects`), and we assume that the directory `experience` is in the *root* of the whole system, then this would be considered as a *full path* and we would navigate to it directly. Then from that end directory (that would mean that we are now in the `projects` directory, which is in the `experience` one), if we navigate with this argument `../skills`, then we would be doing a *relative move backwards* and then one *forwards*. The `..` would indicate that we have to move one parent dir back, and then search in that parent, for a directory called skills. The end result would be an argument that would comprise of the full path to the `skills` sub-tree, which in this case would be `/experience/skills`. This would then be passed to the move command and we would find ourselves in that end destination. Navigating to files would be done in a very similar way.

![side-step](./assets/side-step.jpg "side-step")
Figure 7: Side-Step

Here is a diagrammatic view of this whole process. As we can see, each node is interconnected to the one next to it, and vise versa. For instance, going from `a` to `b` would be simple. All we need to know is that the parent of `a` - `proj`, also is the parent of `b`. Then if we search for `b` in `proj` and find it, we can access and even *go* to `b`. This would be much more efficient than to simply go to each directory, starting from the root, and then making a comparison in order to find what we are looking for. Of course there are some cases, where we have to reach all the way back to the **root**, since our destination is on the other side of the tree. This would be the case where `c` want's to *"see"* `d`.

As a final note, here is an extravagant example that should work. If we take the previous examples mapping, the following argument should be evaluated to a real path and then executed:

``` bash
cd ./awards/../awards/.././education
```

This, by no means of the imagination, is a smart way to navigate places, but by standard `UNIX` functionality, it must work!

- **Directory and file mappings**: Since the application starts off with some pre-generated information, which is provided by the owner of the specific CV, we need to understand how that information is then mapped out into the `VFS`. There are two parts to this algorithm. First, the actual information that needs to be provided needs to be described in terms of paths. This is done through a configuration file, in the base of the whole project. That configuration file, which will be described in greater detail shortly, has a basic mapping on where the needed directories and files need to be. Then in the second part, this configuration file is read by the application and is parsed for these mappings. If the mappings have been correctly addressed, then all of the files on the host system are found and loaded into memory. Then they are mapped in the `VFS` accordingly and are made available for reading and writing. If no such files exists, then the appropriate errors are generated. When the mapping of the files is loaded by the system, for every directory that does not exist, it is created on the spot. If a file is in a directory that does not exist, then that dir is also created on the spot. This mimics the workings of a real directory creation of any `UNIX` system, but in a simpler way. There is no prompt to ask the user, whether or not the missing directory should be created. The reason for this is because it makes the whole application easier to use and simpler to write. *Technically* this is still a valid form of file and directory creation, the only thing that is missing is a prompt!

Let's look at an example. If we have specified that we want to have the following directories present:

`/education /experience /experience/projects /experience/skills /awards` (space separated)

and the following files:

`/education/edu.txt /experience/projects/my_projects.txt /experience/skills/soft_skills.txt /experience/skills/hard_skills.txt /experience/skills/languages/langs.txt` (again space separated)

then we notice that when we are creating `langs.txt`, there is no parent directory `languages` initially. But when the application comes to that point, it will create that directory **automatically**. Then if the user want's to remove that file or dir, he/she can use one of the following commands to do so!

![file-creation](./assets/file-creation.jpg "file-creation")
Figure 8: File Creation

This diagram is a great way to understand the example. We can see that our tree wants to extend all the way down to `c` and `d`, but these places do not exist yet. Well, the `sub-dir` algorithm takes care of that, and when we execute it, we are automatically greeted with *2* new directories!

- **Argument passing**: Since each command takes in some sort of an argument, some algorithm should be made that would recognize these arguments and validate them. Technically, this is a very easy job for this project since each command has it's own argument validation. In real world `UNIX` systems this is most of the time, not the case, and there is a good reason for it. Almost all commands don't only have some argument to be passed to them, but they also have optional arguments as well. Since this project doesn't aim to make a carbon copy of a terminal emulator with a fully working command parsing system, it is assumed that the user passes valid arguments to each command. If something is wrong with the passed data, then an appropriate message will be generated and displayed. Generally, there are two forms of this type of input validation. One is on the client level, and one on the server. When a client enters something that is not valid, i.e. that does not take the form of a real command, then a `mishmash: command not recognized: ' + input.value` is shown. This checks if the command starts with invalid symbols, such as *numbers*, *slashes*, *dots*, and any other *non-standard* characters. If this passes, then the server checks if the input does have such a command. If not, then a `'mishmash: command not found: ' + c` is thrown, where `mishmash` is the name of the **shell**. If all of these things pass, then the designated command with the very first argument is passed to it. After a user has entered a valid command, he/she can continue typing whatever they see fit. That input is taken and is space separated into an array. Then for each command, it's evaluated if that whole array, or only the first element of it, should be passed down. An example would be - if we call the `ls` command like so: `ls /experience/projects /dir1/ /dir2/`, then only the command name and the very first argument would be read, everything else would be dropped. A note should be made about *piping* and *redirection*. The goal of the project is to provide the user with a simple command line interface. Pipping and redirection would make things more complicated and would add functionality which is not needed for the likes of this Interactive-CV. Although it would be a good exercise to make a more complete parser, for now it's considered as an unneeded feature.

- **ls/rm/cd/pwd**: All of these commands interact with the `VFS` directly. They have, as a target, some destination, and depending on the command, they either navigate to it, delete it, or show it's contents. In the case of `pwd`, it just asks the `VFS` for the currently visited dir object. From previous examples, we have a good idea on how these work. All of these use the generic tree traversal algorithm (described above).

- **cat/mkdir/rmdir/touch**: These are very similar to the ones above, but they have a small *caveat*. Since the target is either a new file or directory, when the command handles the parsed argument, it takes the last value of the path array, stores it until a full path has been generated, and then appends it to the end of the full array. That way, the problem **"file not found"** is avoided, since then the command tells the `VFS` to create such a node. In the case where we delete a directory with `rmdir`, instead of creating a directory, we delete it. Otherwise, the implementation is the same. Of course, by the `GNU/Linux` standard implementation, the target directory for deletion **must** be empty beforehand. It's important to note that when a file that already exists is encountered, it is **not** overwritten. This is by standard `UNIX` functionality compatible. None of these commands will override any node, if it has the same name. This minimizes client errors, especially for those who are not used to typing in commands, and is also much more user friendly. As an added benefit, making this functionality actually saved development time and complexity, since the code for it resolves very easily to a simple skip!

- **echo**: The `echo` command is a bit different than the others. There is a duality to its nature, in that it can either simply return whatever has been passed as an argument, or it can add, or append data to any existing file. This is the one command that takes in more than one argument. The first part of the argument is taken to a the string that would be echoed back. Then the whole passed array is checked if it contains one of the two symbols that would indicate data adding or appending - `>` or `>>`. If a single arrow is encountered, then the remaining part of the array is treated like a target file. A file retrieval is issued to the `VFS`, which returns the file object (the `inode`) back, and the new data is added. Note that this approach overwrites any previously existing information in that `inode` (as this is the standard functionality of the echo command in a `UNIX` system). The same process is taken for a *double arrow*, with the difference that instead of overwriting the data of the file, the new data is simply appended to the end.

- **QuickSort/whoami**: One of the stranger commands this app has is the `whoami` one. In a standard system this would only print out the currently logged in user. But this is of no use in this application, so a twist has been made to the command. When this is called, the server responds by printing the personal information of the CV holder to the screen. this information would be data such as email, GitHub account, name and so on. But one of the features this command brings is a sorted list of the 3 most popular repositories the user has in his or her GitHub account. It works like this - when the app is first started, the server issues a request to the GitHub API for the contents of the CVs holder account. A `JSON` is received, comprising of all of the users public repositories. Now there is a tricky part, how are the top 3 picked? By popularity, and more precisely, by star count. All of them are sorted, using the quick sort algorithm, in ascending order. Then the last 3 (the ones with the most stars) are picked out. Since these are repository objects, their name, star count, and link are then issued to the user. It is kind of strange that this whole remote access is done in the start of the application, instead of doing it every time `whoami` is sent, but the reason for this is because of Node JS's `asynchronous` nature. More will be explained in the implementation part of the project.

- **Command Line History**: The command line history is one of the very easy, yet highly useful algorithms that make the whole work with the application much more enjoyable. As a user of a `UNIX` system, having a command line history and access to it is highly important. That's why I implemented a similar thing into this application.

### User Interface
The user interface is mimicking a command line interface, such as the ones that are found in Microsoft's `cmd` or any of the `UNIX`-like terminals. The UI aims at being both readable and easy to understand. That's why on top of the whole page, there is text prompt that encourages the user to type the `help` command in order to orient him/her-self better in the application. The UI is also styled in the way how an old CRT monitor would look like, or an old console would be displayed. The reason for that personal. I find those styles fun to play around with. One of my motivations to go with this is because it's more or less unique and I have used and old **CRT** monitor as a child, which brings back memories. I drew inspiration for this design from a package that is distributed around GNU/Linux distributions, called "cool-retro-term". It tries to bring back the feel of typing on an old *VT100* console. Of course the challenge which comes with this style is that I had to make the text legible and clear, but yet still save the feel of an old CRT screen. Of course this style also contributes towards the overall feeling that the user is talking to a real terminal and not just using a one page web application. If the user is emerged into the world I bring with this style, then they might have fun while exploring the contents of the CV. The most challenging part of the UI was designing the `CSS` styling for it. Since it has to be a cross-browser app, making it work on all major platforms brought a lot of code and styling bloat.

### Software architecture
The software architecture of the application can be considered to be highly modulcaric. What that means is that the application has a *core framework*, which holds everything together. This allows for many small modules to be independently developed and plugged in without the fear that one functionality would break another one. But that's not all, versions controlling these types of projects is also an ease. Different people can work on different plugins at the same time, without bothering each other and most importantly, **not breaking the system** during development. Now since this is also a web application, which makes `HTTP` request to a server, we can also separate this architecture as an MVC, where we have a *Model* which does all of the business logic, a *Controller* that takes in any input and spits out some output, and a *View* that renders everything we see on the screen. The benefit of having more than one view of how an application is structure is that it brings depth to the project, **not** complexity, but depth. When a project is deep we can invest in it with all of our heart. So let's explain how this application works using both of these architectures.

First, let's look at how the MVC side of things would look like.

![MVC](./assets/MVC.jpg "MVC")
Figure 9: MVC

This diagram shows a standard setup for such an architecture. We have a nice controller that distributes the data left and right, keeping everything clean and separated. The Controller is noticed by the *Parser* file. This is just an abstraction, but that parser is the Controller itself. It distributes what we want see with what we are calculating in the back. I have decided to omit the presence of the server file, since the server is extremely small and does not contribute to the overall functionality of the application, so we are not seeing it as another part of the Controller. The View, left of the Controller, is clear - a combination of CSS and HTML with a bit of JavaScript that manipulates the screen of the user. This View is highly self-contained since it doesn't care all that much about the data which is passed. It cares more about how things are aligned on the monitor, which is always a good thing. Finally, we can see the Model below. It's the heaviest part of the application, which is understandable. It has all of the implementation files of the whole system, such as the `VFS`, the trees, the files, the algorithms, etc. It's to our advantage to know that the Model and the View have absolutely no common spots or interactions. Within the whole application there is not one function that comes close to touching the Model side of things. This made it much easier to develop the whole CV!

From the viewpoint of an MVC app, we have a large separation of concerns present. This occurs for two reasons - one is the language of implementation choice, the second is the way the app is developed. The reason that JavaScript allows for an easy separation into Models, View, and a Controller, is because JS has a great way to talk only to the View. It's ease of access to the DOM object allows for fine manipulation of the View. On the server side of things, Node JS  is perfect for implementing the business logic, where large data structures, objects, and algorithms are easy to implement and maintain. And of course, since this is JS, making a Controller is easy! Now what is the View? It is simply one `HTML` page, one styling `CSS` sheet, and one vanilla JavaScript file, that renders the whole View we see when the application starts. Everything that happens on screen is due to this. When we see the monitor expanding and new input boxes appearing, these little parts allow that functionality. Nothing more and nothing less. When a response is generated from the server, that response is sent to the View and it's just rendered on screen. The output is styled due to the `CSS` file present in the project. The Controller is the small parser that takes in a command from the View, separates it into pieces, and then sends it to the command selector, which is part of the Model logic. This allows for different commands to do different things, since not all commands need the implementation of the `VFS`. Some just make a simple calculation or access the time and date. That's why the Controllers job is easy. It just passes information back and forth. The Model is the business logic, or in this case, the CV and `VFS` logic. This is by far the most complex part of the whole application, since creating a virtual file system is not a trivial job. The `VFS`, as already explained on how it works, does all of the computation when a command issues a request for it. Apart from that, the Model reaches out to an outside service and packages that information. Lastly, the Model creates responses in some meaningful and readable format for the user.

Looking at things from the viewpoint of having a modularic structure gives us a different understanding on how the application works. Modularic means that many small modules are combined together into a structure, which in terms builds the whole software. This is very similar as to how operating systems are built. We have a core kernel that is the main framework and many small (or large) commands which comprise the working of the system. In a very similar fashion, Interactive-CV comprises of a core framework - the `VFS`, which allows for any commands to take advantage of it and bring functionality to the project. When one command is implemented, it is completely separate from the workings of another command, That is not to say that one command cannot contact another one, but their independence makes for a more secure and easy to maintain system. This approach brings a lot of benefits to the developer and to the user. On one side, this allows for an easy to implement and easy to maintain software, since when there is a problem, it's easy to understand where it's coming from. If the problem is in the module, the appropriate error message would be a good indicator. If the problem is in the core framework, again it would be noticeable where it comes from. For users it's easy to use such an application, since they don't need to be concerned with the inner workings of the actual core framework. All they care about is that the commands do their jobs. This gives a certain relief to anyone who want's to play around with this application. Finally, such a modular approach gives the robustness and security to the software. When there is a large separation of concerns in a project, there is also clarity and simplicity!

![core](./assets/core.jpg "core")
Figure 10: Core

Much like the standard `UNIX` core diagram, this is the one the application has. It mimics `Linux` by having a central component, to which many small pieces of software can be added. They are the shell that encapsulates the heart of the application. then around it we have an abstraction layer, keeping things nice and clean, and then e have the rest of the application talk to the center, through the provided commands.

Now that we have an overview the general architecture of the Interactive-CV, let's take a closer look at each class and object, and see how it works and what are it's interactions with the rest of the system.

Starting from the front-end, we can see that it comprises of mainly 3 files. The `HTML` page, the `CSS` sheet, and the JavaScript that manipulates the view. The initial `HTML` page is very simple. We are greeted by a welcoming sign, who has developed the project, to what the project contributes to, and most importantly, a simple invitation to the client to type the command "help". If we look at what the JavaScript code does, we would see that it's a purely functional implementation. At this point we have an event listener, that checks if the user has written something into the input box and has also pressed enter. After that happens, a couple of basic checks for correct input are made. If everything is OK, then a POST request with the value of the input box is sent to the running server. At this point the front-end *"freezes"*. JavaScript has a natural way to do things asynchronously, which means that if we do not force the application to wait for a response, then we would have a delayed answer and miss-placed output on the screen. After we receive what the server has to say, we add a new input box, we render the output, we stop the previous event listener and we add a new one to the newly created input. Finally, we also add the last ran command to the command line history queue. All of this is done in a functional way. The only state that is kept is the previous commands. The reason that they are saved in the browser is that this makes for an easier implementation. Also if we are to make the server keep track of that, many unnecessary `POST` requests would have to be sent and respectively gotten when we want to traverse to past commands.

From the client side, we jump on to the server and general workings of the "back-end". Here we have a small server running. It uses the Node JS and Express JS framework, to setup a small server side application. The server itself does nothing more than to keep the state of the static files that are rendered, i.e. the View, and to coordinate the passed commands to the command parser, and vise versa. The server, when started on a `localhost`, runs on port number 3000, and we can access the application by going to `localhost:3000`. Until now, the application is completely procedural, without any objects or asynchronous behavior. That's about to change when we look at the `VFS` and command parser. At this point, we have a lot of objects, each of which has it's own little task.

First, let's look at the user object. This is the representation of the owner of the CV. The user has a basic initialization upon server startup. It reads from the pre-defined configuration file - things like username, GitHub account, name, what will be the mapped out directories and files, and so on. After parsing that data, it does two things - sends the files and directories to the `VFS` to get loaded, and also issues a `GET` request towards GitHub's API. This request then retrieves the designated users repositories and sorts them in order of popularity. At this point, all of the users jobs are finished. From now on, there is only interaction between the `VFS` and the client.

The virtual file system structure comprises of several files. It has a `M-Tree` component, and `iNode` component, and a wrapper object, called jay`VFS`. All of these object create the full `VFS`. Their interaction is as follows - The `M-Tree` and `iNode` objects form the abstract data structure, where they have an inner API, which allows for the outer wrapper object to communicate with them. The wrapper object then places it's own API to any commands that which to interact with the `VFS`. In reality, to command has any direct access to the tree. The `M-Tree` in fact is a completely stateless structure. What that means is that, in no point in time, does the `M-Tree` increment with inner counters, or keep a variable indicating it's state. When doing any traversal, deletion, sub-tree or `iNode` retrieval, or any functionality in fact, the tree has a notion of state only through the base object that is calling it (and since this is implemented in JavaScript, the base object would be referred to with the `this` keyword). The state of the tree is preserved within the jay`VFS` object. In reality, that one instance of the `M-Tree` is it's entire state. So we can say that the tree has a state, but it's functionality is stateless. Working with this type of objects makes life very easy, since there is no way for data or references to get lost (or duplicated for that matter). Whatever we have built from the start, that is what we get in the end in terms of data. The `iNode` object is also part of this structure, but it's sole purpose is to keep state, which is logical, since it represents files. In the end, the jay`VFS` wrapper implements how commands would receive information from these two structures. On it's own, this object would also do some optimizations and filtering. At this point it's adequate to talk about exactly how command options and target file/directories are represented in JS data types and what's the optimizing role of the jay`VFS`. When a command is issued to the server, it is interpreted as plain text. Then it is separated in order to catch the name of the command and the passed argument. That same argument is then split into an array of literal strings. Most of the commands deal with either directory or file targets, so we would have input that looks like this: `ls /experience/skills`. Since the root of the structure is the very first forward slash `/`, and then every subsequent one is used to separate out the rest of the path, there are two approaches that would do the job in resolving the proper file/dir targets. One would be to just clip the very first forward slash, if it is even given as a parameter, and then resolve the rest of the path from there. The other approach is to keep the root and it's string representation separate. What that means is that the root has a conception of it self under some other name, let's say "root", and it's representation would only be a forward slash. I have chosen the latter of the two methods. In the section about implementation, I will talk in greater detail as to why I think this is the better approach, but to summarize here, it allows for a much flexible input handling and a great way of separating a structure from it's representation (separation of concerns). So this would mean that whenever a target is received for some part of the `M-Tree`, it is split into an array of literal strings and this allows for the jay`VFS` to optimize some use cases. Example of this would be if the user want;s to directly jump to the root of the tree, to get the current sub-tree object (which would be the currently visited directory), or if a target file want's to be read. Hierarchically, things would look like this: A client enters a command -> the command parser parses that command -> the jay`VFS` resolves the commands needs with the specified arguments -> the `M-Tree` does some internal manipulation of retrieval of objects -> if necessary, some `iNode` is retrieved all the way back.

It would be a great benefit to imagine how these things work through a component diagram. It's purpose is to show how the different components, be that of classes, functions, or different types of modules, interact with each other and transfer data from one place to another. It will also gives us an understanding of how the different modules transform the data and how it is passed between the back-end and outside world.

![component](./assets/component.jpg "component"){ height=95% }
Figure 11: Component

This is how the component diagram looks like. It's made up of a central, large scale component, namely the Server, which controls all of the inner workings. Inside of it, we have small classes that do a single line of work. The communication between the server and the outside world is done through ports, which are designated *entry* and *exit* points. First we have a User Interface component that is sending data through a port onto a socket that grabs that data and passes it into the path resolution algorithm. This is shown in the upper left side of the diagram. When the data is handed of to the path resolution part, it is evaluated and then sent to the other inner workings of the app. Here several tings get that data. The commands component gets the data and does it's own computation. It can then be done, or pass that information back to the `VFS`. If the file system needs to do so, it goes into the plumbing of the structure and does data manipulation there. It just passed information up and down the structure, where the tree handles everything on it's own. Another part of the commands also access through an exit port a GitHub server through their open API. A response is waited and then a final computation of the data is done. Finally, when an answer is ready to be given, the either the command or the `VFS` generate such a response and send it back up to the client. Again there is an exit port that is used to make that transition from the server out to the world. The idea behind these ports is to make sure that the back-end stays clean from the outside world, and that there is a special place where data comes in. If we allowed for anything and everyone to reach to every single component of the application, complete chaos would ensue. That's why, the Controller side of things handles such data trade-offs, making things more secure and easier to understand.

We can put things into perspective with a deployment diagram, that shows-off how the files are arranged and how the classes of the system interact with each other on this level of abstraction.

![deployment](./assets/deployment.jpg "deployment"){ height=70% }

Figure 12: Deployment

Here we can see that the *UI* set of files form one box that is self contained and is given to the user to access. This module comprises of a client-side JavaScript file, the HTML of the page, and it's CSS styling. That whole group is then interacting with the server module, which is a big grouping of classes and files that do all sorts of different things. Going through them one by one - `VFS` is the virtual file system grouping, which deploys 3 different classes. We have an `mTree` class that represents the whole tree structure of the system. This is were all of the sub-directories and files are held in memory! Then we have an `iNode` class. These are the many different files that are part of the `M-Tree`. They hold the data that is shown on the screen. Finally, we have a wrapper class, the `jayVFS` object, that wraps the two former class into a single structure, that deploys the whole virtual file system. Then we move onto the Parses component. This is just a layered controller between all of the other functionalities. It's a purely functional class that passed data from one point to another, without changing anything. Then we have a User module. This is a separate entity since it deploys a single object that does a lot of things. It maps out the configuration layer of the directories and files to concrete ones that lie on the host system. It sorts incoming data from the GitHub response, and then it is used to generate a tree structure for the `VFS`. All of these responsibilities deserve to be put into a single deployment class, as it is shown. To this, we see that we interpret the remote access to the *GH* servers. This is nothing more than simple data collecting mechanism, but it's part of the whole deployment scheme. Finally, we reach the Commands module. This is a large set of single file, global functions that are able to be accessed within the server and are deployed as accessible commands to the User Interface. This is a highly extensible part of the program, since new files can be added or removed on the fly from the system. As long as there are files on the host that is running the app, the command can be made executable and be offered to the client. This whole picture shows how things are deployed within the software architecture of the whole application, from the start of the *UI*, to the data collection, all the way down to the smallest executable commands on the host server!

This is more or less how the whole structure talks to itself, through the different classes. Lastly, I want to clear things up as to how the commands are resolved. Each command has it's own file. This, in reality, is now real `UNIX` type commands are made. Regardless of their complexity or size, they are separate files that just talk through system calls to the kernel. In the same spirit, each command has it's own structure, data handling, complexity, and implementation, completely separate from the other commands. Most of the commands, especially those that talk to the `VFS`, have the same structure, namely: take in the command line arguments, resolve them to a full path, receive either some sub-tree or `iNode` object, or execute a *move/delete/create* action, and then return a result in the form of a string. An important aspect of these functions is that all of them are both stateless in data handling and in functionality. As with the previous justification for the `M-Tree`, this makes the development and simplicity of the commands a joy to work with. These files are not object, but rather functions made into modules. Since NodeJS allows for such packaging, each command is made like a global stateless function (global as long as it's included in the designated file of course).

**NB**: only one command uses another in order to save on code bloat and code repetition - the echo command uses in itself the touch command in order to resolve any files that need to be created, if the passed arguments ask for that functionality. This is only a single case instance, so it does not really hinder the complexity, or rather, the simplicity of the project. But if more and more commands do this, then a solution is to abstract another layer of common generic commands that do the same thing, where specific commands would use these commons. Such an example would be the common functionality of the path resolution algorithm. This file is such a common functionality that needs to be used in almost all specific commands. Again, this path resolution is both stateless in data and in functionality.

Until know we have a great idea of how data flows inside of the application, how it's handled and what is done with it. But we don't know how an answer is returned to the *UI*. The response structure is very simple - all responses, be that an error message or a valid answer, are in the form of literal strings. That means that in each layer of functionality, there is some sort of try-catch section which checks for validity. If everything passes with flying colors, then a string is return to the user. That string might be an empty string, might be a paragraph, or just a sentence. Of course, due to the nature of front-end JavaScript, there is a small dirty trick we have to do when we return the final answer. If we want a paragraph to be formatted in a certain way, the server, and more precisely - the returning command, must format that paragraph in a valid HTML syntax. This means that we must append breaks, links, and styling extensions to the final string response. As an example, the `whoami` command does that. It appends to each link `<a></a>` tags and to each new line a `<br>` tag. Without these appends to the final response, the *UI* won't render a legible answer to the client.

### Security features
I have briefly described what kind of security measurements have been taken on the different abstraction levels, be that on the algorithm, data structure, or architecture niveau. But let's examine things in detail. In general, there is no database that can contaminated or "broken" by any user input. So the danger of SQL-injections is basically zero. But since this application is working with real files on the system, there is a real problem of deleting files from the actual physical FS. So here is a rundown of the different precaution measurements.

User input on client side of things - at this point the user is allows to type anything into the designated text box in the *UI*. This means that both invalid `HTML`, which could potentially break the View, or some malicious script, for instance written in JavaScript or bash, could be inserted. The safety measures at this point is to make sure that at least some sort of valid text has been written. In `UNIX` type of systems, there exists strange and dangerous commands which can crash a whole system in seconds. One such attack is the famous fork bomb. It's this command - `$> :(){ :|:& };:`. `:()` is a function which gets called recursively from its body and cannot be killed since it is running on the background with `&`. `:` is actually the name of the function. Here is the same function call in human readable format: `forkbomb(){ forkbomb | forkbomb & }; forkbomb`. As we can see the function is calling its self twice in the body. This will start consuming all resources of the host system and eventually force the OS to crash. This is very dangerous if it happens on the application, since it will burn everything to the ground. That's why the client side checks for any non-standard character inputs from the text box. If there are such, then an appropriate error message is shown. If this *"line of defense"* is passed, then the command parser does the rest. The best thing about the server side of security is that nay form of command that is typed, can either have an implementation or it cannot. There are no real command calls to the actual host system so no real harm can be done. Everything is done in RAM, which means that if someone does a major screw-up, then a simple restart will fix things. Of course changes won't be saved, but that's a small price to pay for having a secure system. Each command goes through a custom parses that is on memory level. This means that even the infamous `rm -rf /` command will do nothing on the actual system. It won't even harm the files of the project!

At this point, **no** passwords are handled in the CV app, since this is designed to be an open project that allows for free play. Although this feature can be implemented, a whole lot of code about user identification must be generated in order for some small features that don't really provide any additional benefit to the overall experience.

Also the text that goes back and forth has no real benefit of being encrypted, since all we are doing is passing public data. It would seem strange to keep an open CV encrypted and closed off to anyone who want's to look at it. At this point there is a problem with the tempering of data as someone might change the contents, but that does not pose a real problem to anyone since those changes are not saved anywhere and are in plain memory. On top of this, the whole application is working solely with text based information. Any form of binary data or compiled programs would simply be ignored and won't be able to execute! In fact, if we are to list the permissions of the application in a `UNIX` style, we would have this: `-rw-rw-rw- root root Interactive-CV`.

## 4. Implementation

### Development environment
The whole Interactive-CV is implemented using JavaScript, with the help of the frameworks NodeJS and Express JS. Everything was tested and written under the **Arch Linux** distribution. The fact that the project is about emulating a `UNIX`-like file system and terminal emulator and working under such an environment proved to be quite the advantage. Arch Linux provides a rich learning environment through pushing the user to make manual setups and to experiment with more features that are not present in more user friendly distributions. This OS incentives reading the documentation of standard kernel implementations and inner workings, where through trial and error the client is able to learn a lot about different topics on complex systems, such as file systems and terminal command prompts. For instance, when the initial installation of the system is made, there is a step where the user has to manually create a partitioning scheme and table, onto which the different physical representations of the whole OS would be mapped onto. Then, using that scheme, a file system is manually installed. This whole process allows for a more detail understanding on the communications between an emulated FS and a real one on a live system. Having this information gave me an advantage when building the whole CV, and more specifically, the `VFS`. In addition to this, the development environment composes of two parts. The editor - `Emacs`, and the running simulation and testing ground, which was my hosts terminal emulator - `xterm`. Both of these programs are standard `GNU/Linux` programs. `Emacs` provides a very powerful editing environment, specifically for scripting languages, such as JavaScript. In addition to the bare-bones Emacs, I also use the `JS2` Emacs package that keeps the code in a standardized and modern fashion, where all of the code is checked if it meets modern JS development practices. This goes from code formatting to using proper object oriented construction modules and packaging practices. Such code evaluation packages allow for a more structured development cycle, decreasing the chances of introducing bugs, and increasing the chances of finding previously inserted errors in the code base.

### Languages
Everything in this project is written in JavaScript. The only addition to this are the two server frameworks that are used - NodeJS and Express JS. Node JS is specifically used to handle server side logic, to implement all of the structures and algorithms, and to handle any form of validation. Express JS is used in order to support the **REST**ful web app that is brought up when the application starts. The vanilla JavaScript is solely used to interact with the user input on the front-end of things. It concentrates on expanding the text boxes, creating new HTML elements to the `DOM` object, and listening for new inputs from the user.So why did I concentrate on using Node JS for the core framework and not some other language? The reason I chose this is because I had some basic knowledge in how this structure works. This experience comes from previous projects that I have done in Node. I decided that implementing a more massive and heavy structure would be a great learning experience for me, and to see how things are done in a dynamically typed language. Node JS also allows for accessing the physical FS on the host system, which is a great benefit when reading and writing to actual files through the interface of a `VFS`. Node also has a good modular system, which packages separate files in a neat way, making cross file references a breeze in most cases. Node has a caching system, which allows for the referencing of files. This caching system works like this - if in a local running instance of a node js program, there are several modules, where a module is any object or function (or a set of those) which is specifically set to be part of the global `module` object, then those instances are not re-instantiated to be new objects, but are instead re-referenced. Since JavaScript treats functions as first class objects, meaning that each function is also effectively an object with its own set of methods, I shall be referring to both functions and objects as just *objects*. So the continuous referral to those objects does **not** re-instantiate them, nor does it create new instances. It just returns the original instance. **Notice** - this is very similar to the `singleton` pattern, and it *can* be effectively used like that, but it's easy to break this system due to the internal workings of Node JS. I have taken advantage of this functionality, interpreting some of my inner structures as singletons. This would be the actual `VFS`. But a clarification must be made here. The `VFS` is the abstract concept, and there are 3 files, with 3 objects that comprise the whole data structure. So how is the singleton here managed? The wrapper object - `jayVFS`, is the one that is exported as a singleton. In that object, there is one instance of the `M-Tree` that is solely access by `jayVFS`. When the `M-Tree` has it's sub-directories deleted, or if more are added, then those new sub-dirs are either new object instances of the `M-Tree` class or are just deleted. This means that the `M-Tree` itself is not treated like a singleton outside of the `VFS`, it's just used as a abstract data structure in a functional way. The tree structure is maintained by the root node, which lies in the jay`VFS` singleton instance. In general, it's not always a good idea to have such global singletons, since they might be used to break development practices, but in this case, it makes perfect sense, and not only that, but it drives the whole program to work correctly. All of the commands that are interacting with the `VFS` are in fact referencing this object. If it wasn't a singleton, then all of the commands would be calling new instances of the `VFS`, which would mean that all of the commands would be keeping **different** states about the `VFS`.
Of course this pose it's own set of problems, since in the beginning I wasn't aware of this feature keep to Node JS. I fell exactly into the trap of creating many carbon copies of the `VFS`, without realizing it. In the end, I saw how to fix the problem, which turned out to be an elegant solution to things.

On another side of things, Express JS and Node JS are a great way to setup a small running sever that can interpret `HTTP` requests. The reason that I chose to have a framework that handles my requests and not let Node do it manually is because this is not the point of the project. The application itself does not make a range of requests. It does solely POST actions. So instead of dealing with my own parsing of `HTTP`, it was a better idea to let a framework handle it for me, so I can concentrate of building the actual business logic. Express JS also allows for a very easy way to monitor what is coming to the app and what is going out. Things like static files, which would be the css and html files, are easy to make visible to the framework. This allows for a smoother experience during the overall development.

``` javascript
const express = require('express');

const port = 3000;

let app = express();
app.use(logger('dev'));
app.use(express.json());
app.use(express.static(path.join(__dirname, '.')));
app.use(express.static(path.join(__dirname, 'html')));
app.use(express.static(path.join(__dirname, 'css')));
app.use(express.static(path.join(__dirname, 'src')));
```

This is the whole setup I needed in order to get an Express server running properly on `port 3000`. And here is how I handle all of the `POST` commands that are coming in:

``` javascript
app.post('/', function(request, response) {
    try {
        let r = cparser.parse(request.body.command);
        response.send(r);
    } catch(e) {
        response.send(e.message);
    }
});

app.listen(port);
```

With this, I am able to receive everything the user inputs, and also return some form of a response.

Finally, I want to talk about some of the difficulties I had with JavaScript as a whole. My evaluation of the language for building such massive structures, especially abstract data structures that need to be handled manually, is a hard task. I ran into a big bug during development, which was - I was loosing data from my variables on certain commands. After a lot of digging and searching, I understood that the reason my data arrays were disappearing was that those arrays were loosing they references. See, since everything in JS is an object, when you pass it to a function, the reference of it is passed, **not** a copy. So whenever I went out of the scope of some function, I lost all of my references that were created inside, which meant that some of the data I was holding was also lost. This drove me mad, since I didn't figure out what was happening. This meant that I had to **force** JS to make copies of some of my data. But JS has strange rules on when an object is copied and when it is references. The dynamically typed language *copies* basic data types, literals, numbers and booleans. If an object contains any of these basic data types, then that object is referenced, but when certain functions, such as `slice()`, or `concat()` are used, then that object is copied. But if that object contains any more complex data than basic data types, then there is only a limited amount of ways to make copies - either use some external library that would do the job for you, or use this technique -> `JSON.parse(JSON.stringify(object))`. This would force the object to be made into a string, which would then be re-cast into a copied object. If you are forced to go to such lengths only to copy some array, then the language isn't created for your line of work. Luckily, I was dealing with string literals all of the time, so I had syntactic sugar to help with the copying, but it still looked bad and I had to hide it behind some function.

``` javascript
user.prototype.getDirs = function() {
    return JSON.parse(JSON.stringify(this.dirs));
};
```

This is how I had to make sure that I would get copies of data so it doesn't get lost. I had to hide it in `getter` methods. Here is a quick demonstration as to how JS referencing and copying works:

``` javascript
// Literal values (type1)
const booleanLiteral = true;
const numberLiteral = 1;
const stringLiteral = 'true';

// Literal structures (type2)
const arrayLiteral = [];
const objectLiteral = {};

// Prototypes (type3)
const booleanPrototype = new Bool(true);
const numberPrototype = new Number(1);
const stringPrototype = new String('true');
const arrayPrototype = new Array();
const objectPrototype = new Object(); # or "new function () {}"

// Highest performance for deep copying literal values
arr2 = [...arr1];

// Any of these techniques will deep copy literal values as well,
// but with lower performance.
arr2 = arr1.slice();
arr2 = arr1.splice(0);
arr2 = arr1.concat();
arr2 = JSON.parse(JSON.stringify(arr1));
arr2 = $.extend(true, [], arr1); // jQuery.js needed
arr2 = _.extend(arr1); // Underscore.js needed
arr2 = _.cloneDeep(arr1); // Lo-dash.js needed
arr2 = copy(arr1); // Custom-function needed
```

**Note**: I have not used `Underscore.js` nor `Lo-dash.js`. It was too much work to get the into my project and I don't like dependency bloat.

As a final note to the problems with the language, logging proved to be quite the hassle. Since there were a lot of references that were lost during development, a lot of logs had to be written in many files around the application, in order to understand what was going on. Although a simple log in JavaScript looks like this: `console.log()`, it outputs plain text. Things like date and time of access, code line executed, formatting, and so on, are not included into the package. I come from a `Java` background, where massive applications are logged with special libraries and logging functions, which give you a *detailed* report on what's going on at each and every moment. Things like `log4j` make the life of the developer so much easier and I really missed that in this project. Trying to keep my dependencies list to a minimum, it was suffering to sometimes find out how to log things properly in order to understand what kind of output you are getting. Being able to **read** the program is half the battle, *debugging* it is the other half!

### Installation requirements
Installing the application is a fairly simple process. There are several ways to obtain the code - either through GitHub, or through some physical form, i.e. like a disk or USB medium. After the code is obtained, it can be run with the node modules package manager `npm`. The target file that starts the program is `server.js`. The whole process can be summarized into the following steps:

``` bash
# Get source code
git clone https://github.com/Anarcroth/interactive-cv.git

# Move to active directory
cd interactive-cv

# Start the application
npm server.js
```

This will create a local server on port `3000`. If you navigate to `localhost:3000` in your browser, then you will see the welcoming screen of the app. The default setup for the application can be found in the `.conf` folder in the base of the Interactive-CV directory. This files contains all of the data that can influence the functionality of the app. More precisely speaking, here you can setup your personal *GitHub* account and can specify how you want to map out your files and directories. In order for things to work properly, you have to fill in any contents you want to be shown in the `contents` directory (also in the base dir of the project). The those files have to be mentioned in any way you want them to be shown in the *UI*. If you miss to mention some files in the configuration file, then they won't be shown on screen. If you specify non-existent one, the will be created but left empty. Finally, you are left with the freedom to host this project wherever you see fit. There are many free platforms that allow for Node projects to exist, such places are - `Kanban`. Of course you can take the task of hosting thing yourself!

**How to read .conf**
The file itself has an explanation for every field present. The most important thing is to keep the format of the files, since it's parsed in a specific way in the app. Failure to keep the format of the configuration will lead to **undefined** behavior. Here is the basic layout of the file:

``` text
# This file is auto-generated by defualt.

# Username
username = 'Your name'

# Email
email = 'youremail@example.com'

# Prompt
prompt = '$ > '

# GitHub account name
github = 'YourGitHub'

# Structure of the virtual file system.
# The root directory is always `/`.
# All subdirectories start from the root, unless specified otherwise.
# If a parent directory is not found, then it is automatically created
# with a base at the root.
# All directories must start with `/`!
sub_directories = /education /experience /experience/projects
/experience/skills /awards

files = /education/edu.txt /experience/projects/my_projects.txt
/experience/skills/soft_skills.txt /experience/skills/hard_skills.txt
/experience/skills/langs.txt /experience/exp.txt /awards/awards.txt
```
### Fragments and interesting implementations
Some parts of the code hold interesting insights as to how things work exactly. The most fun parts are the `VFS` implementation, some parsing functions, and some structural decisions. As we will see, the `M-Tree` has a recursive nature to it, as with any tree structure, while the other setup functions and algorithms follow a more linear implementation. Having recursive implementations makes things a bit more complex to understand, but also cut down on code bloat and make code more concise. So let's look at some snippets and explain what's going on.

**`M-Tree` navigation**

``` javascript
mTree.prototype.moveTo = function(dir) {
    let workingDir = dir.shift();
    if (dir.length === 0) {
        let nextSubtree = this.subtrees.find(s => s.name === workingDir);
        if (!nextSubtree) {
            throw Error('directory doesn\'t exist.');
        }
        return nextSubtree;
    } else {
        let nextSubtree = this.subtrees.find(s => s.name === workingDir);
        return nextSubtree.moveTo(dir);
    }
};
```
This is one of the most referred to functions in the project. Movement across the tree is vital, so this function is called by a lot of methods and higher level functions. `moveTo` is part of the `M-Tree`. It takes in an array of literal strings, representing the full path to a target directory. The first element of the array is extracted. Then, if there are no more elements in the array, it means that we have reached the target destination the used desired. A check is made if the `workingDir` is present in the current sub-tree that we have visited. This sub-tree can be the root or some other directory down the hierarchy. If it is, then the whole sub-tree object (which is an `M-Tree` object) is returned. If the initial array is not empty however, then this means that the `workingDir` exists and we can go to the next element of the array and do the same checks with the next sub-directory. If an invalid path has been passed, then we can see that a `Error('directory doesn\'t exist.')` will be generated. This error will be caught and re-thrown up the chain, all the way to user level.

**`M-Tree` directory creation**

``` javascript
mTree.prototype.addSubtree = function(dir, parent) {
    let workingDir = dir.shift();
    if (this.subtrees.find(s => s.name === workingDir)) {
        if (dir.length === 0) {
            throw Error('File exists');
        } else {
            let nextSubtree = this.subtrees.find(x => x.name === workingDir);
            nextSubtree.addSubtree(dir, nextSubtree);
        }
    } else {
        this.subtrees.push(new mTree(workingDir));
        let nextSubtree = this.subtrees.find(x => x.name === workingDir);
        nextSubtree.parenttree = parent;
        nextSubtree.fullpath = nextSubtree.getFullPath(nextSubtree);
        if (dir.length > 0) {
            nextSubtree.addSubtree(dir, nextSubtree);
        }
    }
};
```

This function shows us how to create new directories in the tree. As with the previous function, we take in the current `wokringDir`, extracted from the end of the passed array. In addition to this, we also have a parent argument. This parent indicates which was the previous object that called the function. Since in NodeJS the `this` keyword referrers to the current object that is executing the commands, we have no notion of the previous object that was used. That's why we pass a reference with this argument. But why do we need to know the parent? Because this will guarantee us a backwards access to the tree. If we have lateral movement up the tree to the root, we can also access the other sub-dirs children, which would then allows us to make side steps from the current directory. Of course, we check if we have reached the end of the search for the destination. If not, we call the same function again, but this time we pass as a parent the next working directory, to ensure we have a backwards window opened. This creates a circular dependency in the referencing table in the NodeJS environment, but the platform is smart enough to notice this and handles it appropriately. If we output the structure, we would see how it has a `[Circular]` member for the `this.parenttree` data member. This does not break the system, it just tells us which object lies in this variable. If we access that object manually, we would in consequence be reaching out to the parent tree. When we add new directories, we generate a new `M-Tree` object, populate it with it's parent and it's full path for ease of use and we continue down the recursive chain until the target directory array is empty.

**Creating files**

``` javascript
mTree.prototype.addINode = function(node) {
    let inode = this.inodes.find(i => i.name === node);
    if (inode) {
        return inode;
    } else {
        let newNode = new iNode(node);
        this.inodes.push(newNode);
        return newNode;
    }
};
```

Of course we should also look at how files are added. This function takes in just a name of a file. It is expected that `this` would be the wanted target directory object in which we want to add the file. If such a file name already exists, then we just return that `iNode` object. This is, by `UNIX` standards, a valid implementation. Data loss is horrible, so it's better to not override a file if it exists. It's also not expected that an error would be raised if a file already exists. If we look at the actual implementation of the `touch` command, we would see that it only changes the timestamp of a file without changing it's contents, so that's what we are also doing here.

But why does this command expect that `this` would be the target sub-tree, instead of passing an absolute path, navigation to it, and then creating the file there?
The reason for this is because navigation and file creation are two different things. If we want to create a file, we just want to create it, if we move up and down the tree, we might introduce referencing bugs. The implementation of this functionality in the wrapper layer looks like this:

``` javascript
jayVFS.prototype.createFile = function(filePath) {
    try {
        if (filePath[0] === '/') {
            return this.mtree.addINode(filePath.pop().toString());
        } else {
            let file = filePath.pop();
            let tarDir = this.mtree.moveTo(filePath);
            return tarDir.addINode(file);
        }
    } catch(e) {
        console.log(e);
        throw e;
    }
};
```

This function is the one that any command call if it wants to create a file. As we can see, here is the place we navigate to using the standard navigation functions, and then, once we have the desired tree object, we create a file inside of it. This is much cleaner and easier to read, than re-implementing the same traversals in the `addINode` function. This also has a nice separation of implementation, where functions just do one thing and one things only!

Since we are on the wrapper level now, let's look at some key implementations that hold the whole things together.

There is one **important** concept about the whole navigation system. We have to know where we are at all times. Since we have relative paths, we need to understand where the user is at any point in time. This is done through the internal variable `this.wd`. It's an array keeping the full path to the currently visited directory. Every time we navigate somewhere, this variable changes. This working directory allows us to quickly understand where we are, instead of having to recalculate our current position each time we need it (and we need it a lot!).

So navigation on the wrapper level would look like this:

``` javascript
jayVFS.prototype.moveWdTo = function(dir) {
    if (dir[0] === '/' && dir.length === 1) {
        // This might need 'root' as a value.
        this.wd = ['/'];
    } else if (dir[0] === '.' || dir[0] === './') {
        // Do nothing.
    } else {
        try {
            this.wd = this.mtree.moveTo(dir).fullpath.slice();
        } catch(e) {
            console.log(e);
            throw Error(e.message);
        }
    }
};
```

We can see that we always, regardless of what we have as a passed path, change the state of `this.wd`. This sate is so important to us, that we have a `getter` method that we use abundantly -> `jay`VFS`.prototype.getWd`.

Also here is how we read the contents of any tree object.

``` javascript
jayVFS.prototype.getDirContents = function(dir) {
    let lsContents = [];
    let movedToDir = {};
    if (dir[0] === '/' && dir.length === 1) {
        movedToDir = this.mtree;
    } else {
        try {
            movedToDir = this.mtree.moveTo(dir);
        } catch(e) {
            console.log(e);
            throw Error(e.message);
        }
    }
    movedToDir.subtrees.forEach(d => lsContents.push(d.name));
    movedToDir.inodes.forEach(i => lsContents.push(i.name));

    return lsContents.filter(n => n);
};
```

Here we expect an absolute path to be given as the `dir` argument. If we have to return `root`, we do so, otherwise we move our tree to the desired directory and from that object we extract it's data members. Specifically all it's sub-dirs and files. We package them and return them to the calling command. Although this implementation is trivial, there are a lot of things to be careful about, such as the first `if` check. We do those two checks, because the passed dir array might contain a `root` indication but also it might mean more than that, so we have to see if the user only meant the root or something more.

Before we continue on to check how the path resolution algorithm works, I just want to point out how we create the `jayVFS` singleton object.

``` javascript
module.exports = new jayVFS();
```

This line guarantees us that we have only one single instance of the object exported to the global module space, which will always be references by any other function or object.

Until now we have seen these target arrays, representing directories and file targets, but how are they actually made? With the `relativePathResolution.js` file.

``` javascript
let resolve = function(tarDir) {
    let d = [];
    if (tarDir.startsWith('/')) {
        d.push('/');
    }
    d = d.concat(tarDir.split('/').filter(n => n));
    if (d === undefined || d.length === 0) {
        return jvfs.getWd();
    } else {
        return [...getAbsPath(d)]; // return a copy of the array
    }
};
```

The file has some interesting functionalities. For instance, this here makes the general decision on how to handle relative paths. Here we would expect that there are some relative symbols in the passed target path. The `tarDir` parameter is the actual string that is written by the user, when he/she were executing some command. First, we check if the string starts with a forward slash `/`. If it does, we append it to a temporary array. Then we split the string based on `/` symbols, where between each forward slash, we would expect to have a directory. If that produces an empty string, then we know that the passed in argument is referring to the current directory. Why is that? Because if the string starts with a forward slash and has *nothing* else, then we are referring to the **root** dir. At this point, this *must* be also the current dir. This is a strange distinction since we have to have a way to differentiate between when a user means **root** and when current dir. So we have this check to help us out. If the user meant only for *root*, then we *wouldn't* have another guess besides the current dir. But if the user wants another path, that is relative, then this check would be skipped. After that we continue onto moving up and down. Since this is a relative path, we check if we need to move backwards (i.e. up the tree hierarchy), or forwards. There are three cases we need to cover.

**1:** We want to move forward from the current tree. If that's the case, we indicated it by the lack of `..` in the path.
**2:** We have to move up the tree to parent directories. In this situation, we have `..` present.
**3:** We both want to move back and then forwards. This is equivalent to moving to the side of some directory. So first we execute step **2**, and then if we have more of the path that's left, we go to step **1**.

``` javascript
let getAbsPath = function(d) {
    let wdStree = [];
    if (d[0] === '/') {
        wdStree = jvfs.getRoot();
    } else {
        wdStree = jvfs.getWdSubtree(jvfs.getWd());
    }
    let wd = wdStree.fullpath.slice();
```

Here we prepare to execute the algorithm. From the previous function, we check the state of the directory, by either getting the root or some other one. In both cases, we should end up with some array of literal strings, representing some `sub-dir`.

``` javascript
for (let i = 0; i < d.length; i++) {
    if (d[i] === '.') {
        // do nothing
    } else if (d[i] === '..') {
        if (wdStree.parenttree.name === 'root' || wdStree.name === 'root') {
            wd = ['/'];
            wdStree = jvfs.getWdSubtree(wd);
        } else {
            wd = wdStree.parentree.fullpath.slice();
            wdStree = jvfs.getWdSubtree(wd);
        }
    } else {
        if (wdStree.subtrees.find(s => s.name === d[i])) {
            wd = wdStree.fullpath.concat(d[i]);
        } else {
            wd.push(d[i]);
        }
    }
}
```

This code snippet is a continuation from the above one. At this point, we go through the whole range of the sliced up string array, where at each point we evaluate a complete path. Now, one might think that evaluating a complete path at each point would be cumbersome and time consuming, but that's not the case here! Since we have *instant* access to each parent and sibling, and also to their `fullpath`s, we can instantly build up an absolute path for each iteration.

At this point we complete all of the **3** steps. First we check for any current dots. They meaning nothing to us, so we just skip them. Then if we have a step **2** movement, we move back, relative to our current location. If at some point, we move so far back that we reach the **root** node, we simply overwrite our whole destination with the destination of the root! Of course, if afterwards we are to move forwards somewhere, that won't be a problem, since we evaluate the next path with ease. We just find the next `sub-tree` and concatenate it's full path. If we can't find anything, then we just *append* the passed argument at that point. **Note** that this would result in an error later on, when the `VFS` tries to evaluate this invalid path, but that's what we want!

``` javascript
    // ... more code above
    return wd;
};

```

Finally, we return the final form of the absolute path and return it to the command that issued it!

In order to connect all of the dots together, let's see how one command uses all of these functionalities together. Here is the `mkdir` command:

``` javascript
let mkdir = function(paramDir) {
    let tarDir;
    if (!paramDir) {
        return 'mkdir: missing operand';
    } else {
        tarDir = paramDir.split('/').filter(n => n);
        let tempDir = tarDir.pop();
        if (tarDir.length < 1) {
            tarDir = jvfs.getWd();
        }
        tarDir = rpr(tarDir);
        tarDir.push(tempDir);
        tarDir = tarDir.filter(n => n !== '/');
    }
    try {
        jvfs.makeDir(tarDir);
        return '';
    } catch (e) {
        return 'mkdir: cannot create directory \'' + paramDir + '\': ' + e.message;
    }
};
```

Here we take in the parameter passed by the user. We extract the target directory name that is to be created. Then we resolve the `tarDir` array. We have no idea if this is a relative path or not, but the `rpr(tarDir)` command is actually the interface to the whole path resolution mechanism. Then receive from it an absolute path and tell the `VFS` to create the directory.

Here is a complete example:

``` bash
# Check if which directory we are
pwd
/projects/skills

# Make a directory into a side folder
mkdir ../../awards/testDir

# See the result
ls ../../awards
testDir
awards.txt
```

### Libraries and frameworks
As already mentioned, Node JS and Express JS are our friends in this project.

## 5. Testing
Testing this project was quite the challenge, since there are a couple of levels on which one can break the system. As with the previous sections, I will start with the front-end of things. This is web application, which means that whatever we see, needs to be properly rendered and interpreted by the application. For this reason, my testing environment is both *Google-chrome* and *Firefox*. I'm not using other browsers since they are not as popular and I am counting on the fact that the standard functions I use in the front-end JS will work correctly. The problem with the browsers is two fold - show what is meant to be shown and function in the way it's meant to be used. All of the different browsers have different implementations of the same functions, HTML, and CSS styling. Which means that whatever works for *Firefox* might not work for google-chrome. Of course, these two big browsers are very compatible, but there are also some use cases where some web-based application work only for one browser. So how does this application compare to looks on both browsers? Fairly well I would say. A side by side comparison yields that some blur is lost in Google-chrome, but in general, it looks exactly the same way. So the test of the eyes holds up. But what about the functionality? After going through all of the commands and command combinations in both browsers, it's safe to say that the JavaScript implementation is cross platform compatible. This is expected, since when the JS functions were picked initially, I chose to use those which have a standard implementation on all browsers, exactly for cross-browser compatibility.

Going to the back-end of things, more rigorous tests should be applied here. The catch here is that we cannot trust that if a single command does what it's meant to do, it won't break the other part of the system. That's why a through combination between commands should be applied here. Over the course of testing the application, I saw that there were some problems with the tree structure, which were cause by the innervation of one command towards another. That's why I made my software architecture to be as decoupled as possible. For instance, one of the bugs caused undefined behavior when navigating to another sub-tree. The reason - I was loosing references when passing data. My acceptance tests were as follows - see what's the functionality of a command in a standard `UNIX` environment, and then see that the projects implementation for that command behaves in the same way. This means that for each command I implement, it has to be the same as the standard one in a real OS. There was an acceptance test for each command present. Then there was an acceptance test for the whole structure, for it to not loose data or references. Finally, there were acceptance tests for each file and directory that was created, deleted, or altered.

Let's look at the tests for the whole structure:

- **Keep the state of the `VFS` clean and defined**: This was achieved by rigorous logging mechanism and reference counting. For each action that was taken in the UI, there was a log that indicated what was happening at each phase of the command parsing and response. The test is passed only when the final state of the `VFS` has a logical connection with the actual representation on the screen and in the mind of the user. Things like missing references, incorrect or circular dependencies, missing object connections, missing object references, and missing circular references, were considered to break the system and yield **undefined behavior**. The tests were done as follows - for each functionality of the `VFS`, there was representation of the tree before and after the function executed. If the state of the tree coincides with reality, then the test passes, if not, it fails. Finally, a generic test, that goes through the whole structure and through each functionality has to depict a valid tree.
An example of the test would be as follows:
If a directory is created in a relative way, then that directory should exist in memory in the designated spot. Then navigating to the new place should not leave the old one undefined. When in the new directory, all access to any other point in the tree should be valid. This kind of sequential commands should always result in a successful execution.

- **Navigation of the tree should not break it**: This test was created so that whenever the state of the current working directory does not break or loose reference. This test was difficult to pass, since there were a lot of ways for data loss, due to the inner implementation of how Node JS does array copying. In order for this to pass, each time there is a move command, the tree has to have a valid path that is the one shown on screen. Moving backwards, forwards, and to the sides should always end up with a valid target and a valid general state, regardless if the move is done in either an absolute or relative way.

*Example*:
When navigation to the end of a directory, the navigation should work with either a full path or a relative one. If a relative one is used, any form of a relative path should be valid, regardless if it's written with a dotted notation or not.

**Lateral navigation test**

``` bash
cd /experience/projects

# The above should be equal to the bottom example

cd exprience # or cd ./exprience
cd projects  # or cd ./projects
```

**Horizontal navigation test**

``` bash
cd /experience/projects
cd /awards

# The above should be equal to the bottom example

cd exprience # or cd ./exprience
cd projects  # or cd ./projects
cd ..
cd ../awards
# or
cd ../../awards
```

- **Proper error messages should always tell the truth**: `UNIX` systems have a great way to tell the user if he/she has done something that is not correct. The important thing is that, if a user does an invalid command, it should not leave the state of the system unusable. When a command want's to do something that is not allowed, a stack trace should be generated in the back-med logs, while a concise and appropriate message should be given to the client. Furthermore, the test should also be valid if and only if the state of the program does not change upon encountering and error.

*Example*:
When a user want's to create a file inside a non-existent directory, then that's OK. The missing directories should be created in real time and then the file should also be generated in the end. This is a valid implementation by `UNIX` standards. If a user want's to access a file in a non-existent directory, then that should generate an error message to the console window. If a user want's to navigate to a impossible place, that should also show an error. If a user enters an invalid command, then that should be stated to him/her. Such cases are endless and there are many ways to break the system. Here is how the errors are shown to the user:

``` bash
ls invalidDir
ls: cannot access 'invalidDir': directory doesn't exist.

cd invalidDir
cd: no such file or directory 'invalidDir': directory doesn't exist.
```

All of the commands follow a hierarchical generation of errors, where at each level, the error message is wrapped in an appropriate header style. The end result should indicate which command was executed, what is the cause, and what generated the error.

Also entering invalid states should be handled by each commands implementation standards. This means that for the same passed argument, different exceptions should be thrown.

``` bash
# Wrong!
rmdir /
rmdir: cannot delete root directory!

# Correct!
ls /
education
experience
awards
```

Finally, the different levels of error handling should also be present. For instance, the entering of a wrong command.

``` bash
notvalidcommand
mishmash: command not found: notvalidcommand
```

All of these tests should guarantee that the system is always operational and **not** left broken. Of course, it is expected that the server would recognize these errors, and would leave a complete stack-trace that would indicate with accuracy where the problem occurred. This is a desired and wanted behavior as part of the test, since it will make debugging easier for the developer. A valid stack-trace should look like this on the server side of things.

``` bash
Error: directory doesn't exist.
    at mTree.moveTo (/interactive-cv/src/mtree.js:62:19)
    at jay`VFS`.moveWdTo (interactive-cv/src/jvfs.js:71:34)
    at cd (interactive-cv/src/commands/cd.js:19:14)
    at parser.parse (interactive-cv/src/parser.js:33:16)
    at interactive-cv/server.js:27:25
    at Layer.handle [as handle_request] (interactive-cv/node_modules/express
        /lib/router/layer.js:95:5)
    at next (interactive-cv/node_modules/express/lib/router/route.js:137:13)
    at Route.dispatch (interactive-cv/node_modules/express/lib/router/route.js:112:3)
    at Layer.handle [as handle_request] (interactive-cv/node_modules/express
        /lib/router/layer.js:95:5)
    at /interactive-cv/node_modules/express/lib/router/index.js:281:22
```

- **Indefinite file support**: Since we are testing the capabilities of a `VFS`, another acceptance test is to check how many files the `VFS` can support at anyone time. For this reason, a small unit test was written in order to see how much files can a directory hold. If a directory can hold up to 10 000 files, then that should be enough for the usage of the application. Having more of that is considered unreasonable by the needs of this project. Here is how the test is implemented.

``` javascript
// [TEST]
// Create 10 000 files
this.create10000();

// [TEST Implementation]
jayVFS.prototype.create10000 = function() {
    for (let i = 0; i < 10000; i++) {
        this.createFile(['awards', 'file' + i]);
    }
};
```

And we can check the result we have!

``` bash
ls awards
file0
file1
file2
file3
file4
file5
file6
file7
file8
file9
file10
file11
file12
...
...
...
file9999
```

![tenk](./assets/100000.jpg "tenk")
Figure 13: 10 000

Since everything seems to be working fine, we can call the test a success!

I made a second test that tried to stress test the system a little bit more. In a similar fashion, I created a test function that would create *1000* directories on *root* level. Then in each directory, I would create *1000* files and fill them up with the complete documentation (in fact, this very file) in raw text. The test looks like this:

``` javascript
// [TEST]
this.create1mil();

// [TEST Implementation]
jayVFS.prototype.create1mil = function() {
    // Read complete documentation
    let lines = fs.readFileSync('./README.md', 'utf8', function(){});

    // Create 1000 directories
    for (let j = 0; j < 10000; j++) {

        this.mtree.addSubtree(['dir' + j], this.mtree);

        // Create in each directory 1000 files
        // all filed with the whole documentation
        // of this project
        for (let i = 0; i < 10000; i++) {
            let f = this.createFile(['awards', 'file' + i]);
            f.append(lines);
        }
    }
};
```

Unfortunately, after running the test, my host machine started heating up pretty quick due to the overwhelming amount of computation I put on it. I didn't see a *spike* up in memory usage, which is normal since text is usually small in *byte size*, but after a while of waiting, I decided to terminate the test. Although this test doesn't really strive to prove any real usability of the application, it's still interesting to see that the server didn't crash, it just took too long to create *1 million files with a lot of text inside each of them*. I doubt that such a use case would occur, but the experiment was fun to do!

## 6. Results and Conclusion
After all of this, we see that the end product is a interactive console, which allows for anyone to go around the little project, play with the commands, read, and most importantly, get in contact with the candidate. If I were to compare the current results with what I envisioned, I'd say I came close to my expectations. Of course the work never stops and there are countless ways in which this project can be expanded. I think that my vision for how the program should look like and how it should feel, came pretty close.

![compare](./assets/compare.jpg "compare"){ height=170%; }
Figure 14: Comparison

We can see that the terminals are virtually indistinguishable. Of course, the reason we have a missing heart from the Google-Chrome browser is because some character sets are not always rendered perfectly on different platforms. Accounting for those things is very difficult, especially for non-experts.
The style of the terminal is something that is more or less an original idea, while the feel when you type and the answers you get are also nice touch to the overall experience. I had an idea to make this project help out anyone who want's to stand out of the crowd and I believe that this can be achieved. All of the text is readable, easy to access and of course, fun to play around with.

### Work done

From all of the implemented features, everything works. Refactoring is a second hand job, it's left for a time where stress is not a factor in development. But just because things are working as planned, doesn't mean that I am happy with it's full implementation. For instance, I would like to refactor how I do the relative path resolution, since it can be made to work in a more concise and recursive way. I feel like I did a good job with the whole modularic layout of the architecture. After some core bugs were cleaned out, the rest of the development was really enjoyable to do. I guess that there are some bugs here and there, but after all of the things, I think they are pretty hidden. Naturally, this doesn't meant that my work is done. I will gladly continue developing this project and even use it for my own personal needs. The CV doesn't have a lot of commands, in terms of quantity, but each command works pretty well and their overall quality more than justifies their number.
Over the course of development, a lot of problems were encountered. Not only with the language of implementation, library, architectural, but also setup, configuration, and general course of action problems were had. As stated in the previous sections about the curiosities in Node JS, problems with missing references and loss of data ate away a lot of my time. It is very difficult to work with languages which implement inner garbage collection and hidden references that you can't always be sure where they are falling off. As an example, the first time I encountered such a problem was when I was developing the `ls` command. It was only working for the root directory, but not for any other. So I had to dissect all of my functions in order to understand where the problem was coming from. In the end I saw that some of the arrays were missing data, which led me to think that I might be dealing with missing pointers. Lo and behold, that wast he cause of the bug. Another problem which I caught early on in the project were the many copies I was creating of the `VFS`. I saw that each command had it's own `VFS` instance, instead of referencing only one global object. At that point I read the documentation on Node JS about modules specifically and I learned how to use the modules object to my advantage. Apart from the depths of server side JavaScript, I also encountered problems with the Express JS library. Since I have static files, the CSS and HTML ones, I wanted them to be recognized by the server. In the beginning that wasn't happening and I couldn't get access to my View. This of course was an easy fix, after I read the documentation on the library, but in the beginning it did thrown me for a loop. Other than these problems, I was also fighting against the asynchronous nature of Node JS. I know it's created with the idea to be completely asynchronous in it's work, but I didn't need that functionality, so I had to circumvent some of it's default capabilities. This is one of the reasons why I calculate the best repositories of the user in the beginning of the app, because it was very hard to make the call to GitHub synchronous. Apart from source code, it was difficult to decide how to make the configuration file and how to setup the whole project in terms of structure. I knew this decision is important to get right from the beginning, since later on I won't have the chance to make re-arrangements in the architecture. Luckily I managed to get my modules right!

### Screenshots

Here are a few screenshots of the running application, both taken from the front-end back-end of the program. They show that in fact things are working and being access appropriately!

![index](./assets/index.jpg "index")
Figure 15: Index

This is the first thing we see when we open our browser to the destination of the app. We are greeted with the author, the name of the project, and a prompt as to how to start the application. We can also notice that all of the text is readable and clear!

![basic](./assets/basic.jpg "basic")
Figure 16: Basic Intro

Jumping right into the action, we have already executed a few commands. First we `ls` the current directory (view it's contents) and then we navigate to where the `educaiton` folder is. We pint out our current location with `pwd`. Finally we can see what's inside of that place, and namely - the file `edu.txt`.

![help](./assets/help.jpg "help")
Figure 17: Help

Here we can see that if we call the `help` command, we would be given a complete list of all of the commands that we can execute. To each one, there is a small description that gives us the general feel of the command.

![help2](./assets/help2.jpg "help3")
Figure 18: Help2

Since the output is too long, I've cut it in half not to overwhelm. But, if we try to execute the `help` command with an additional argument to it, we get this:

![help3](./assets/help3.jpg "help3")
Figure 19: Help3

It gives us a more detailed explanation of the command, plus, what kind of augments can it take. Specifically, for these two, the `cd` command takes in any directory argument, while the `whoami` does not need to have anything added to it.

![cat](./assets/cat.jpg "cat")
Figure 20: Cat

On this screenshot we can see how e can read the data from a file onto the console. All of the text is properly formatted and fitted onto the screen for ease of use.

### Missing features

There are a ton of things that I wish I had the time to do. Maybe in first place I would put the need for sessions and user authentication. This project would be great if each viewer had his/her own session. In addition to this, being able to log-in to your files through the interface and re-arrange things would be very fun and much more secure than leaving things in the open air. Since the project was about implementing virtual systems, such things as sessions and authentication weren't a top priority, but they are still quite important. Another feature I wanted to add was the need for client side cookies, which would be used to hold temporary data in the form of files. Each time a client would look at the CV, he/she could make files and then they would be saved in their browser cache. Finally, I want to add much more commands. This is just the beginning, and this structure can do so much more. Commands that implement file renaming, deeper FS access and more file types, just to name a few, are on my to-do list. I also want to have access to the Stack Exchange sites, in order to get more relevant data of the user. The problem with that is that, I need to apply for an access token in order for the app to be able to call the public Stack Exchange API. If I have such a token, then I can make up to 10 000 requests a day in order to get my profiles data. In addition to this, I wish I had the time to deploy this project on some hosting site, such as `Kanban`, where Node JS applications can live. For a free, I can make this my permanent website, and I plan on doing so indeed in the near future! Having this type of application accessible to the mass public is crucial to get yourself known. It will benefit for job searching and connections.

### What I learned

What I got from the project? Many things. I got experience in working with a framework I knew little about. I got to emulate concepts that I find interesting. I learned a lot about the inner workings of `UNIX` systems, how they implement file systems and how they handle different layers of abstractions. I learned how to deal with dynamically typed languages, how to debug then and make the more resistant to bugs. It learned also how to meet deadlines and go for project meetings when it is required. One of the most invaluable things that I saw is that at some point, development must stop and preparation for the final presentation must begin. It's all so very tempting to continue working on the things you like, but at some point you have to pull the breaks and deal with the rest later. I saw that some features would require too much time to develop, regardless of how much I would have liked to have them, and dealing with that realization is a difficult thing. I also learned how to always keep a log of the progress I've made over the course of a week. Knowing that you are always doing some small bits of progress is always a motivation, especially when there are a lot of bugs.

On the more technical side of things, I learned that dealing with dynamic languages has it's own benefits and weaknesses. On one side, they are a great way to jump start a project, create a fast demo, do something on the fly, be quick and dirty for most things, make a proof of concept, or some other small project that is cute and manageable. But when things get out of hand with file count and code size, then things get difficult exponentially. I was shocked how difficult it was to manage logs properly. Most languages, such as Java, with a couple of frameworks, such as `log4j`, allow for an amazing time in debugging. Everything is properly aligned and arranged. But in such projects, where the longs are nothing more than simple text, then things tend to get messy quickly.

\ \
\ \
\ \
\ \
\ \
\ \
\ \
\ \
\ \
\ \
\ \

## 7. References
Here is a list of references that helped out through development of this project. There references were used to gain a better understanding of how real world system and file managements actually work. Some of them are also language specific, such as how Node JS works on the inside.

[File System Implementation:](http://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf)
http://pages.cs.wisc.edu/~remzi/OSTEP/file-implementation.pdf

[Directory indexing:](http://ext2.sourceforge.net/2005-ols/paper-html/node3.html)
http://ext2.sourceforge.net/2005-ols/paper-html/node3.html

[The File System:](http://www.science.unitn.it/~fiorella/guidelinux/tlk/node94.html)
http://www.science.unitn.it/~fiorella/guidelinux/tlk/node94.html

[Merkel Tree:](https://en.wikipedia.org/wiki/Merkle_tree)
https://en.wikipedia.org/wiki/Merkle_tree

[ZFS:](https://en.wikipedia.org/wiki/ZFS)
https://en.wikipedia.org/wiki/ZFS

[BTRFS:](https://en.wikipedia.org/wiki/Btrfs)
https://en.wikipedia.org/wiki/Btrfs

[GNU/Linux Operating Systems I:](http://teaching.idallen.com/cst8207/12f/)
http://teaching.idallen.com/cst8207/12f/

[Unix/Linux File System and Pathnames:](teaching.idallen.com/cst8207/12f/notes/160_pathnames.html)
teaching.idallen.com/cst8207/12f/notes/160_pathnames.html
